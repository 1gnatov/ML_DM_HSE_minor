{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/beeline_data_school_logo.png\">\n",
    "# \"Прикладной анализ данных\"\n",
    "#### Интенсивный курс по изучению машинного обучения и анализа данных\n",
    "<img src=\"../../img/beeline_logo.jpg\" height=\"240\" width=\"240\">\n",
    "## Автор материала: преподаватель Факультета Компьютерных Наук НИУ ВШЭ Кашницкий Юрий\n",
    "</center>\n",
    "Материал распространяется на условиях лицензии <a href=\"https://opensource.org/licenses/MS-RL\">Ms-RL</a>. Можно использовать в любых целях, кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 13. Продвинутые методы классификации и регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Использование API Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sys\n",
    "sys.path.append('../../scripts/')\n",
    "from load_titanic_with_features import load_titanic\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyBlackBox(BaseEstimator):\n",
    "    def __init__(self, base_classifiers={GradientBoostingClassifier(): {},\n",
    "                                        RandomForestClassifier(): {},\n",
    "                                        LogisticRegression(): {}},\n",
    "                                        verbose=True, n_jobs=-1, cv=5):\n",
    "        self.base_classifiers = base_classifiers\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.cv = cv\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return BaseEstimator.get_params(self, deep=deep)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        return BaseEstimator.set_params(self, **params)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf_weights = {}\n",
    "        for clf in self.base_classifiers:\n",
    "            print(clf)\n",
    "            params = self.base_classifiers[clf]\n",
    "            current_best_clf = GridSearchCV(clf,\n",
    "                                 params,\n",
    "                                 verbose=self.verbose, n_jobs=self.n_jobs, cv=self.cv)\n",
    "            current_best_clf.fit(X, y)\n",
    "            self.clf_weights[current_best_clf.best_estimator_] = current_best_clf.best_score_\n",
    "        print(self.clf_weights)\n",
    "\n",
    "    def predict(self, X):\n",
    "        final_predictions = np.zeros([X.shape[0], 1])\n",
    "        sum_clf_weights = sum(self.clf_weights.values())\n",
    "        clf_weights = [weight / sum_clf_weights\n",
    "                       for weight in self.clf_weights.values()]\n",
    "\n",
    "        for clf in self.clf_weights:\n",
    "            final_predictions += self.clf_weights[clf] / sum_clf_weights * clf.predict(X).reshape([X.shape[0], 1])\n",
    "        # print(final_predictions)\n",
    "        return (final_predictions > 0.5).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Fitting 3 folds for each of 960 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  12 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2450 jobs       | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2874 out of 2880 | elapsed:  1.0min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done 474 out of 480 | elapsed:   29.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   30.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{GradientBoostingClassifier(init=None, learning_rate=0.3, loss='deviance',\n",
      "              max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=3, min_samples_split=1,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=90,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False): 0.85185185185185186, LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0): 0.8125701459034792, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False): 0.83164983164983164}\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Fitting 3 folds for each of 960 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  12 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2450 jobs       | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2874 out of 2880 | elapsed:  1.1min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 474 out of 480 | elapsed:   38.6s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   39.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0): 0.81818181818181823, GradientBoostingClassifier(init=None, learning_rate=0.2, loss='deviance',\n",
      "              max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=2, min_samples_split=1,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=90,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False): 0.83838383838383834, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=4, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False): 0.84175084175084181}\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 474 out of 480 | elapsed:   35.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   36.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Fitting 3 folds for each of 960 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2450 jobs       | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2874 out of 2880 | elapsed:  1.1min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=3, min_samples_split=1,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False): 0.81313131313131315, LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0): 0.8158783783783784, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False): 0.81818181818181823}\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   36.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Fitting 3 folds for each of 960 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2450 jobs       | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2874 out of 2880 | elapsed:  1.3min remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0): 0.80134680134680136, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False): 0.82659932659932656, GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=4,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=90,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False): 0.82659932659932656}\n",
      "CPU times: user 1min 8s, sys: 3.64 s, total: 1min 11s\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, y, X_test = load_titanic(\"../../data/titanic_train.csv\",\n",
    "                                \"../../data/titanic_test.csv\")\n",
    "\n",
    "\n",
    "forest_params = {'criterion': ('gini', 'entropy'),\n",
    "                 'n_estimators': list(range(50, 300, 50)),\n",
    "                 'max_depth': list(range(1, 5)),\n",
    "                 'min_samples_leaf': list(range(1, 5))}\n",
    "\n",
    "gboost_params = {'learning_rate': [0.1, 0.2, 0.3],\n",
    "                 'n_estimators': list(range(10, 100,20)),\n",
    "                 'max_depth': list(range(1,5)),\n",
    "                 'min_samples_leaf': list(range(1,5)),\n",
    "                 'min_samples_split': list(range(1,5))}\n",
    "\n",
    "log_reg_params = {'C': [0.1, 5, 10, 50]}\n",
    "\n",
    "clf = MyBlackBox(base_classifiers={GradientBoostingClassifier(): gboost_params,\n",
    "                                   RandomForestClassifier(): forest_params,\n",
    "                                   LogisticRegression(): log_reg_params},\n",
    "                     cv=3)\n",
    "\n",
    "clf.fit(X_train, y)\n",
    "\n",
    "scores = cross_validation.cross_val_score(clf, X_train,\n",
    "                                          y, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830527497194\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "predicted_df = pd.DataFrame(predictions,\n",
    "                            index = np.arange(892, 892 + X_test.shape[0]),\n",
    "                            columns=[\"Survived\"])\n",
    "predicted_df.to_csv(\"../../output/titanic_myblackbox.csv\", \n",
    "                    index_label=\"PassengerId\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
