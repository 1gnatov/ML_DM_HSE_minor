{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ml_theme.png\">\n",
    "# Майнор \"Интеллектуальный анализ данных\" \n",
    "# Курс \"Введение в анализ данных\"\n",
    "<img src=\"../../img/faculty_logo.jpg\" height=\"240\" width=\"240\">\n",
    "## Авторы материала: преподаватели ФКН НИУ ВШЭ Кашницкий Юрий и Козлова Анна\n",
    "</center>\n",
    "Материал распространяется на условиях лицензии <a href=\"http://www.microsoft.com/en-us/openness/default.aspx#Ms-RL\">Ms-RL</a>. Можно использовать в любых целях, но с обязательным упоминанием автора курса и аффилиации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинары 7 и 8. Теория вероятностей и математическая статистика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Часть 2. Оценки плотности распределений, метод максимального правдоподобия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видели, байесов подход к классификации состоит в том, чтобы искать класс, максимизирующий произведение **функции правдоподобия** класса на **априорную вероятность** класса.\n",
    "$$\\Large a(x) = \\arg max_{y} \\frac{p(x|y) P(y)}{p(x)} = \\arg max_{y} p(x|y) P(y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность Байесова подхода в том, что мы, как правило, не знаем $P_y$ и $p_y(x)$, поэтому их приходится _оценивать_.\n",
    "\n",
    "* $P_y$ для каждого класса $y \\in Y$ можно оценить следующим образом:\n",
    "\n",
    "$\\hat{P}_y = \\frac{l_y}{l},$ где\n",
    "\n",
    "$l_y$ — количество объектов класса $y$ в обучающей выборке,\n",
    "\n",
    "$l$ — общее количество объектов в обучающей выборке.\n",
    "\n",
    "По закону больших чисел (ЗБЧ) данная оценка будет стремится к априорной вероятности класса при увеличении обучающей выборки.\n",
    "\n",
    "* Существуют **параметрический** и **непараметрический** подходы к оцениванию функции правдоподобия класса.\n",
    "\n",
    "1) Параметрический подход предполагает, что распределение объектов данного класса взято из некоторого заданного семейства (например, нормальное распределение). Таким образом, необходимо лишь оценить его параметры (например, при помощи метода максимального правдоподобия).\n",
    "\n",
    "2) Непараметрический подход — нечто близкое к построению гистограммы, однако чуть более усложненной (об этом далее).\n",
    "\n",
    "Конечно же, методы оценки плотностей распределений активно используются в статистике (здесь просто мотивацией послужил байесов подход к классификации)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод максимального правдоподобия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть имеется некоторая выборка $x_1, x_2, ..., x_n$ из заданного распределения с неизвестным параметром. Как оценить значение параметра?\n",
    "\n",
    "$$\\Large L(x_1, ..., x_n| \\theta) \\to \\max_{\\theta},$$\n",
    "\n",
    "где $L(...)$ — функция правдоподобия выборки, $\\theta$ — параметр, который необходимо найти.\n",
    "\n",
    "Предположим, что выборка независимая. Тогда функция правдоподобия запишется следующим образом:\n",
    "\n",
    "$$\\Large L(x_1, ..., x_n| \\theta) = \\prod_{i = 1}^n p(x_i|\\theta).$$\n",
    "\n",
    "Чаще всего вместо задачи максимизации функции правдопобия удобнее решать задачу максимизации её логарифма:\n",
    "$$\\Large \\ln L(x_1, ..., x_n| \\theta)  = \\sum_{i = 1}^n \\ln p(x_i|\\theta) \\to \\max_{\\theta},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример.**\n",
    "\n",
    "Пусть имеется выборка $x_1, ..., x_n$ из нормального распределения. Найдите оценку максимального правдоподобия параметров $\\mu, \\sigma.$\n",
    "\n",
    "**Решение.**\n",
    "\n",
    "Запишем логарифм функции правдоподобия для нормального распределения:\n",
    "\n",
    "$$\\Large L(x_1, ... x_n|\\mu, \\sigma) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp (- \\frac{(x_i - \\mu)^2}{2 \\sigma^2}),$$\n",
    "\n",
    "$$\\Large \\ln L(x_1, ... x_n|\\mu, \\sigma) = \\sum_{i=1}^n ( - \\frac{1}{2} \\ln 2 \\pi - \\ln \\sigma - \\frac{1}{2 \\sigma^2} (x_i - \\mu)^2) = -\\frac{n}{2} \\ln 2 \\pi - n \\ln \\sigma - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n (x_i - \\mu)^2.$$\n",
    "\n",
    "Как искать максимум? Необходимое условие экстремума функции $f(x)$ в точке $a$: $\\frac{\\partial f}{\\partial x} (a) = 0.$\n",
    "\n",
    "$$\\Large \\frac{\\partial L}{\\partial \\mu} (...) = - 0 - 0 - (-1) \\cdot \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^n 2 (x_i - \\mu) = \\frac{1}{\\sigma^2} \\sum_{i = 1}^n (x_i - \\mu) = 0.$$\n",
    "\n",
    "$$\\sum_{i=1}^n x_i - n \\mu = 0,$$ $$ \\sum_{i=1}^n x_i = n \\mu,$$ $$\\mu_{MP} = \\frac{1}{n} \\sum_{i=1}^n x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Непараметрические оценки плотности распределения\n",
    "\n",
    "\n",
    "Будем рассматривать одномерную выборку $(x_1, ... x_m), x_m \\in \\mathbb{R}$ из распределения с плотностью $p(x)$.\n",
    "\n",
    "1) Дискретный случай\n",
    "\n",
    "Пусть $X$ — конечное множество. Тогда оценкой плотности может служить гистограмма значений:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{l} \\sum_{i = 1}^m [x_i = x]$$\n",
    "\n",
    "2) Непрерывный случай\n",
    "\n",
    "Пусть $X = \\mathbb{R}.$ Тогда предыдущую оценку можно немного модифицировать:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{l \\cdot h} \\sum_{i = 1}^m [|x - x_i| < h],$$\n",
    "\n",
    "где $h > 0$ — параметр, _ширина окна_.\n",
    "\n",
    "3) Локальная непараметрическая оценка Парзена-Розенблатта\n",
    "\n",
    "Оценка из предыдущего пункта является кусочно-постоянной. Чтобы избежать этого, используют следующую оценку:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{l \\cdot h} \\sum_{i = 1}^m K(\\frac{x - x_i}{h}),$$\n",
    "\n",
    "где $K(z)$ — функция, называемая ядром.\n",
    "\n",
    "Таким образом, для восстановления плотности распределения в точке $x$ используются лишь те точки обучающей выборки, которые попали в окно заданной ширины $h$ вокруг $x$, причём чем ближе точка к $x$ — тем выше её вклад.\n",
    "\n",
    "<img src = \"../../img/kernels.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
